<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Gullal S. Cheema</title> <meta name="author" content="Gullal S. Cheema"/> <meta name="description" content="Exploring the AI landscape for social good"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://gullalc.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/lists/">Lists</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Gullal S. Cheema </h1> <p class="desc">Affiliations - TIB, L3S Research Center, Leibniz University Hannover</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> Visual Analytics, TIB Lange Laube 28, 30159, Hannover, Germany </div> </div> <div class="clearfix"> <p>I am a fourth year PhD student at Leibniz University Hannover, Germany and work as a researcher in the <a href="https://fakenarratives.github.io/" target="_blank" rel="noopener noreferrer">Fake Narratives</a> project. Prior to this, I was a <a href="https://ec.europa.eu/research/mariecurieactions/node_en" target="_blank" rel="noopener noreferrer">MSCA</a> fellow and worked on one of the projects in <a href="http://cleopatra-project.eu/" target="_blank" rel="noopener noreferrer">CLEOPATRA</a> ITN. I am employed as a research assistant at <a href="https://www.tib.eu/en/research-development/visual-analytics/" target="_blank" rel="noopener noreferrer">Visual Analytics Group</a>, TIB and advised by <a href="https://www.tib.eu/en/research-development/visual-analytics/staff/ralph-ewerth/" target="_blank" rel="noopener noreferrer">Dr. Ralph Ewerth</a>. I closely collaborate with <a href="https://sherzod-hakimov.github.io/" target="_blank" rel="noopener noreferrer">Dr. Sherzod Hakimov</a> (Now at University of Potsdam), <a href="https://scholar.google.de/citations?user=Ian_NPUAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Dr. Eric Müller-Budack</a> and <a href="https://scholar.google.de/citations?user=hL-N3mUAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Christian Otto</a>. My current work and research revolves around interdisciplinary topics like <strong><em>deep/machine learning, multimodality with focus on Computer Vision and NLP, semiotics and digital humanities</em></strong>.</p> <p>My research on multimodality spans across challenging applications like claim, hate-speech, sentiment and out-of-context media detection. My thesis focuses on multimodal alignment, cross-modal relations and semiotics in the context of multimodal media content in the news domain.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 12, 2024</th> <td> Received the <em><strong>Best Paper Award</strong></em> at ICMR 2024 for the paper <strong>“Identification of Speaker Roles and Situation Types in News Videos”</strong> </td> </tr> <tr> <th scope="row">Apr 7, 2024</th> <td> Two papers accepted in ICMR’24 (ACM International Conference on Multimedia Retrieval). <ul> <li><a href="https://dl.acm.org/doi/10.1145/3652583.3658101" target="_blank" rel="noopener noreferrer">Identification of Speaker Roles and Situation Types in News Videos</a></li> <li><a href="https://dl.acm.org/doi/10.1145/3652583.3657622" target="_blank" rel="noopener noreferrer">Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict</a></li> </ul> </td> </tr> <tr> <th scope="row">Jan 7, 2024</th> <td> 3rd edition of MUWS’24 workshop proposal accepted in ACM International Conference on Multimedia Retrieval (ICMR) 2024, Phuket, Thailand. <a href="https://muws-workshop.github.io/" target="_blank" rel="noopener noreferrer">Call for papers</a> (Deadline - 14th April) </td> </tr> <tr> <th scope="row">Oct 21, 2023</th> <td> Proceedings of the MUWS’2023 workshop: <a href="https://ceur-ws.org/Vol-3566/" target="_blank" rel="noopener noreferrer">Proceedings</a> </td> </tr> <tr> <th scope="row">Sep 11, 2023</th> <td> Overview of the CLEF-2023 Checkthat! Lab on Checkworthiness, Subjectivity, Political Bias, Factuality, and Authority of News Articles and Their Source. <a href="https://link.springer.com/chapter/10.1007/978-3-031-42448-9_20" target="_blank" rel="noopener noreferrer">Paper</a>, <a href="https://ceur-ws.org/Vol-3497/paper-019.pdf" target="_blank" rel="noopener noreferrer">Task 1 overview</a> </td> </tr> <tr> <th scope="row">Jul 16, 2023</th> <td> <a href="https://muws-workshop.github.io/cfp/" target="_blank" rel="noopener noreferrer">Call for papers</a>! <em>Workshop proposal accepted in CIKM 2023. MUWS 2023 - The 2nd International Workshop on Multimodal Human Understanding for the Web and Social Media.</em> <strong>Deadline: 18th August, 2023</strong> </td> </tr> <tr> <th scope="row">Jun 22, 2023</th> <td> Dataset paper of global news coverage on Russo-Ukrainian conflict: 1.5 million tweets from Feb 2022 to May 2023, 3000+ news twitter handles and 60 languages. <strong><a href="https://arxiv.org/abs/2306.12886" target="_blank" rel="noopener noreferrer">Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict</a></strong> </td> </tr> <tr> <th scope="row">May 10, 2023</th> <td> End of the evaluation cycle on Checkthat! Lab at CLEF 2023. Task-1 on unimodal and multimodal checkworthiness estimation received submissions from 14 teams. Team submission papers, task and lab overview papers to be out in July. </td> </tr> <tr> <th scope="row">Apr 14, 2023</th> <td> Gave a talk on multimodal sentiment analysis and claim detection on social media <strong>@BreGroMM</strong> (THE BREMEN-GRONINGEN ONLINE WORKSHOPS ON MULTIMODALITY) [<a href="https://docs.google.com/document/d/1I0-KBshqERN9HkV2voTMDYn4lvj5QrhDZXqLbTYc3ko/edit?pli=1" target="_blank" rel="noopener noreferrer">Program</a>] [<a href="https://docs.google.com/document/d/1mieEZHAdDw2D_QTftdM0W5qn-QQsKpEEA7LTaku3Qx0/edit" target="_blank" rel="noopener noreferrer">Abstract</a>] </td> </tr> <tr> <th scope="row">Mar 22, 2023</th> <td> Journal paper accepted in Frontiers in Artificial Intelligence - Language and Computation. <strong><a href="https://www.frontiersin.org/articles/10.3389/frai.2023.1125533/abstract" target="_blank" rel="noopener noreferrer">Understanding Image-Text Relations and News Values for Multimodal News Analysis</a></strong> </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICMR</abbr></div> <div id="DBLP:conf/mir/CheemaATBEM24" class="col-sm-8"> <div class="title">Identification of Speaker Roles and Situation Types in News Videos</div> <div class="author"> <em>Gullal S. Cheema</em>, Judi Arafat, Chiao-I Tseng, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'John A. Bateman, Ralph Ewerth, Eric Müller-Budack' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/TIBHannover/SRR_NSR_News_Videos" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The proliferation of news sources on the web amplifies the problem of disinformation and misinformation, impacting public perception and societal stability. These issues necessitate the identification of bias in news broadcasts, whereby the analysis and understanding of speaker roles and news contexts are essential prerequisites. Although there is prior research on multimodal speaker role recognition (mostly) in the news domain, modern feature representations have not been explored yet, and no comprehensive public dataset is available. In this paper, we propose novel approaches to classify speaker roles (e.g., "anchor," "reporter," "expert") and categorise scenes into news situations (e.g., "report," "interview") in news videos, to enhance the understanding of news content. To bridge the gap of missing datasets, we present a novel annotated dataset for various speaker roles and news situations from diverse (national) media outlets. Furthermore, we suggest a rich set of features and employ aggregation and post-processing techniques. In our experiments, we compare classifiers like Random Forest and XGBoost for identifying speaker roles and news situations in video segments. Our approach outperforms recent state-of-the-art methods, including end-to-end multimodal deep network and unimodal transformer-based models. Through detailed feature combination analysis, generalisation and explainability insights, we underscore our models’ capabilities and set new directions for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Frontiers</abbr></div> <div id="cheema6understanding" class="col-sm-8"> <div class="title">Understanding Image-Text Relations and News Values for Multimodal News Analysis</div> <div class="author"> <em>Gullal S. Cheema</em>, Sherzod Hakimov, Eric Müller-Budack, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Christian Otto, John A Bateman, Ralph Ewerth' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Frontiers in Artificial Intelligence</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.frontiersin.org/articles/10.3389/frai.2023.1125533/full" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>The analysis of news dissemination is of utmost importance since the credibility of information and the identification of disinformation and misinformation affect society as a whole. Given the large amounts of news data published daily on the Web, the empirical analysis of news with regard to research questions and the detection of problematic news content on the Web require computational methods that work at scale. Today’s online news are typically disseminated in a multimodal form, including various presentation modalities such as text, image, audio, and video. Recent developments in multimodal machine learning now make it possible to capture basic “descriptive” relations between modalities–such as correspondences between words and phrases, on the one hand, and corresponding visual depictions of the verbally expressed information on the other. Although such advances have enabled tremendous progress in tasks like image captioning, text-to-image generation and visual question answering, in domains such as news dissemination, there is a need to go further. In this paper, we introduce a novel framework for the computational analysis of multimodal news. We motivate a set of more complex image-text relations as well as multimodal news values based on real examples of news reports and consider their realization by computational approaches. To this end, we provide (a) an overview of existing literature from semiotics where detailed proposals have been made for taxonomies covering diverse image-text relations generalisable to any domain; (b) an overview of computational work that derives models of image-text relations from data; and (c) an overview of a particular class of news-centric attributes developed in journalism studies called news values. The result is a novel framework for multimodal news analysis that closes existing gaps in previous work while maintaining and combining the strengths of those accounts. We assess and discuss the elements of the framework with real-world examples and use cases, setting out research directions at the intersection of multimodal learning, multimodal analytics and computational social sciences that can benefit from our approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div> <div id="cheema-etal-2022-mm" class="col-sm-8"> <div class="title">MM-Claims: A Dataset for Multimodal Claim Detection in Social Media</div> <div class="author"> <em>Gullal S. Cheema</em>, Sherzod Hakimov, Abdul Sittar, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Eric Müller-Budack, Christian Otto, Ralph Ewerth' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jul 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.findings-naacl.72/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://aclanthology.org/2022.findings-naacl.72.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/tibhannover/mm_claims" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">BIGMM</abbr></div> <div id="shukla2020semi" class="col-sm-8"> <div class="title">Semi-supervised clustering with neural networks</div> <div class="author"> Ankita Shukla, <em>Gullal S. Cheema</em>, and Saket Anand</div> <div class="periodical"> Jul 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1806.01547" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.computer.org/csdl/proceedings-article/bigmm/2020/09232516/1o56CFBx6Ks" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Clustering using neural networks has recently demonstrated promising performance in machine learning and computer vision applications. However, the performance of current approaches is limited either by unsupervised learning or their dependence on large set of labeled data samples. In this paper, we propose ClusterNet that uses pairwise semantic constraints from very few labeled data samples (&lt;5% of total data) and exploits the abundant unlabeled data to drive the clustering approach. We define a new loss function that uses pairwise semantic similarity between objects combined with constrained k-means clustering to efficiently utilize both labeled and unlabeled data in the same framework. The proposed network uses convolution autoencoder to learn a latent representation that groups data into k specified clusters, while also learning the cluster centers simultaneously. We evaluate and compare the performance of ClusterNet on several datasets and state of the art deep clustering approaches. </p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%75%6C%6C%61%6C.%63%68%65%65%6D%61@%74%69%62.%65%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=t1MvIUQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/gullalc" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/gullal-cheema-872b4a55" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/gullal7" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> You can reach out to me at gullalcheema@gmail.com or gullal.cheema@tib.eu </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Gullal S. Cheema. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>